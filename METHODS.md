# Methods

### Initial Exploratory Phase

##### Acquire the genome
Use the _Ranitomeya imitator_ genome found at: 
       
    /mnt/lustre/macmaneslab/shared/Stuckert/Stuckert_Ranitomeya_imitator_genome

##### Find Duplicates

Run BUSCO on the genome and view the [full_table_Ranitomeya_imitator_genome_BUSCO.tsv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/full_table_Ranitomeya_imitator_genome_BUSCO.tsv) file to pull out gene duplications.

    See the script: Initial_BUSCO_run_pull_outDups.sh (Below)
   [Initial_BUSCO_run_pull_outDups.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/Initial_BUSCO_run_pull_outDups.sh) 

Pull out only the duplicated genes from the full summary table and concatinate them into a single file
    
    grep "Duplicated" full_table_Ranitomeya_imitator_genome_BUSCO.tsv > duplicated_BUSCOoutput.tsv

##### Preliminary Analysis

Open [file generated by the grep command](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/duplicated_BUSCOoutput.tsv) in Excel

Add the column headers in the file as such: 

    # These mirror the BUSCO column headers

    BUSCO ID    Status    Scaffold    Start    End    Score    Length

Delete string characters around each of the numbers in the saffold column.

    Find and replace ->  "scaffold" replace with nothing
    Find and replace ->  "_pilon_pilon" replace with nothing
    
    #This leaves behind only the numbers which I can work with in R
    #Save as a CSV, TSVs are a bit cumbersome
    
The table now looks like this: [duplicated_genes.csv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/duplicated_genes.csv)
    
Use R to determine statistics about the dataset

    See the script: initial_R_dataAnalysisAndFigures.R (Below)
    
[initial_R_dataAnalysisAndFigures.R](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/initial_R_dataAnalysisAndFigures.R)

View the CSV generated by R with the Number of Deplicates per Contig Here: [duplicates_perContig.csv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/duplicates_perContig.csv)

##### Try to do a test alignment with two genes found very far away from each other

Used the BUSCO ID and Scaffold from the [full_table_Ranitomeya_imitator_genome_BUSCO.tsv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/full_table_Ranitomeya_imitator_genome_BUSCO.tsv)

Sampled a gene that was duplicated twice on scaffolds that were far apart in terms of the order they were assembled

       # In this case it was on scaffolds 9 and 6848
       # I need to unwrap the genome inorder to grep the sequence from it
       
       awk '{if(NR==1) {print $0} else {if($0 ~ /^>/) {print "\n"$0} else {printf $0}}}' imitator.1.3.6.fa > unwrapped_imitator.1.3.6.fa
       
     
________________________________________________________________________________________________________________________________________
**_This was used to align an entire scaffold which is largely impractical. Don't need to do this, can just pull out the region of interest_**

       # I pulled out each of the scafold chunks and put them into their own files
       
       grep "scaffold9_pilon_pilon" -A1 unwrapped_imitator.1.3.6.fa > scaffold9_pilon_pilon.fa
       grep "scaffold6848_pilon_pilon" -A1 unwrapped_imitator.1.3.6.fa> scaffold6848_pilon_pilon.fa
       
       # I cat them together
       
       cat scaffold6848_pilon_pilon.fa scaffold9_pilon_pilon.fa > MAFFTtrial.fa
       
Take this file and test run a MAFFT alignment

    See the script: MAFFTtrial.sh (Below)
    
[MAFFTtrial.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/MAFFTtrial.sh)

________________________________________________________________________________________________________________________________________
**_I did it again but this time with only the portion of the scaffold that was of interest_**

Need to load linuxbrew/colsa so I can use bedtools [getfasta](https://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html) function

       bedtools getfasta will extract the sequence defined by the coordinates in a BED interval 
       and create a new FASTA entry in the output file for each extracted sequence. By default, 
       the FASTA header for each extracted sequence will be formatted as follows: 
       “<chrom>:<start>-<end>”.
       
Command:

       bedtools getfasta -fi unwrapped_imitator.1.3.6.fa -fo regions_of_interestTEST.fa -bed id_file.tsv
       
       -fo	Specify an output file name. By default, output goes to stdout.
       -fi    Input fasta file, i used the unwrapped genome
       -bed   ID file that is required with gene coordinates
       #For -bed I pulled out the given beginning and ending coordinates of the gene of interest

See the ID file (-bed): id_file.tsv (Below)
    
[id_file.tsv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/id_file.tsv)

Take the bedtools output fasta file and test run a MAFFT alignment

    See the script: MAFFTtest_regions_of_interest.sh (Below)
    
[MAFFTtest_regions_of_interest.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/MAFFTtest_regions_of_interest.sh)

**View Alignment Here:** [test_alignment_of_EOG090701ET.aln](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/test_alignment_of_EOG090701ET.aln)

**View on NCBI Alignment Viewer:** [NCBI Alignment View](https://www.ncbi.nlm.nih.gov/projects/msaviewer/?key=phY8y7oUZcG5Slu-mlttuM0omiildtd53XTYXclIzlv_y84R8vfLRqqpH50JPlDLEpsJxxrYPYI3xC7RJNYh2yXoD9QG-TnS,oBA6zbwSY8e_TF24nF1rvssunC6gcNJ_2HLdW8xOy136zcsX9_HOrP7xtcX4IMZbhAufV4xIqxKhVLhBska3S7N4mUSQaa9C)
________________________________________________________________________________________________________________________________________
**_I did this a third time but this time I extended the region of interest 3kb in either direction to gather an idea for what is on either side of the gene_**

Need to load linuxbrew/colsa so I can use bedtools [getfasta](https://bedtools.readthedocs.io/en/latest/content/tools/getfasta.html) function
       
Command:

       bedtools getfasta -fi genomeFiles/unwrapped_imitator.1.3.6.fa -fo regions_of_interestTEST-3kb.fa -bed id_file-3kb.tsv
       
       See the ID file (-bed): id_file.tsv (Below)
    
[id_file-3kb.tsv](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/id_file-3kb.tsv)

Take the bedtools output fasta file and test run a MAFFT alignment

    See the script: MAFFTtest_regions_of_interest-3kb.sh (Below)
    
[MAFFTtest_regions_of_interest-3kb.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/MAFFTtest_regions_of_interest-3kb.sh)

**View Alignment Here:** [test_alignment_of_EOG090701ET-3kb.aln](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/test_alignment_of_EOG090701ET-3kb.aln)

**View on NCBI Alignment Viewer:** [NCBI Alignment View-3kb](https://www.ncbi.nlm.nih.gov/projects/msaviewer/?key=1WVPuMlnFrLKOSjN6Sgey75b6xAHTnVBf0x6ZWtwbGNd82wr1a7sSwypRp1GYJR81izNcN5v-TXzc-pm4GHlbOFfy2PCTv1l,xHReqdh2B6PbKDnc-DkP2q9K-gEIX3pQcF11dGRhY3JS4mM62r_jXr0M-DjKMY10zyTUeMdn4D3qe_Nu-Wn8ZPhX0mvbRuRt)

________________________________________________________________________________________________________________________________________
________________________________________________________________________________________________________________________________________

### Analyze the depth of the sequencing:

#### Analyze the depth of the sequencing for the Nanopore reads

Run a BWA alignment and use samtools depth to check the depth

    See the script: NanoporeBWAandDepth.sh (Below)
    
[NanoporeBWAandDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeBWAandDepth.sh)

#### Analyze the depth of the sequencing for the Illumina reads

Run a BWA alignment and use samtools depth to check the depth

    See the script: IlluminaBWAandDepth.sh (Below)
    
[IlluminaBWAandDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaBWAandDepth.sh)

#### Analyze the depth of the sequencing for the PacBio reads

Run a BWA alignment and use samtools depth to check the depth

    See the script: PacBioBWAandDepth.sh (Below)
    
[PacBioBWAandDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/PacBioBWAandDepth.sh)

### Look for over representation in reads (example: Nanopore)

       #Reduce the file to just be the contigs with a depth larger than 8
       awk -v x=8 '$3 >= x'  nanoDepth.tsv > nanoDepthgreaterthan8.tsv
       
       #Sort Reduced File
       sort -k 3,3  nanoDepthgreaterthan8.tsv > nanoDepthgreaterthan8SORTED.tsv
       
### Filter Nanopore data to see if it reduces the crazy over representation

Run samtools view command to filter

       See the script: NanoporeFilter-f4.sh (Below)
      
[NanoporeFilter-f4.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeFilter-f4.sh)

Do it again with different flag and compare filtering ability

       See the script: NanoporeFilter-f256.sh (Below)
      
[NanoporeFilter-f256.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeFilter-f256.sh)

representation
      
[IlluminaFilter-f4.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaFilter-f4.sh)

Do it again with different flag and compare filtering ability

       See the script: IlluminaFilter-f256.sh (Below)
      
[IlluminaFilter-f256.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaFilter-f256.sh)

### Run statistics on the Nanopore data of the new filtered data

Run samtools flagstat command

       See the script: NanoporeFlagstat-f4.sh (Below)
      
[NanoporeFlagstat-f4.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeFlagstat-f4.sh)

Do it again with different the data from the 256 flag

       See the script: NanoporeFlagstat-f256.sh (Below)
      
[NanoporeFlagstat-f256.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeFlagstat-f256.sh)

### Run statistics on the Illumina data of the new filtered data

Run samtools flagstat command

       See the script: IlluminaFlagstat-f4.sh (Below)
      
[IlluminaFlagstat-f4.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaFlagstat-f4.sh)

Do it again with different the data from the 256 flag

       See the script: IlluminaFlagstat-f256.sh (Below)
      
[IlluminaFlagstat-f256.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaFlagstat-f256.sh)

### Run Depth on the filtered sam files (Nanopore)

Run samtools depth command on both files

       See the script: Nanopore_postfilterDepth.sh (Below)
      
[Nanopore_postfilterDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/Nanopore_postfilterDepth.sh)


### Run Depth on the filtered sam files (Illumina)

Run samtools depth command on both files

       See the script: Illumina_postfilterDepth.sh (Below)
      
[Illumina_postfilterDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/Illumina_postfilterDepth.sh)

### I found out the filters didnt seem to do much. So I just proceeded with the regular depth TSV files
#### I then ran some stats to see what the files look like

This is the script I ran for Illumna, I did the Nanopore with the same procedure though it was on the head node instead in a tmux window

       See the script: IlluminaDepthStats.sh (Below)
      
[IlluminaDepthStats.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaDepthStats.sh)

I ran another command in a tmux window to sort the file to get the min and the max depth

       sort -k 3,3  IlluminaDepth.tsv > illumina_readsDepthSORTED.tsv

### I wrote a script to calculate summary stats for the depth files

I initially did it in bash which worked but for larger files it ran out of memory and it took a long time to run.

       awk '{sum+=$3;a[x++]=$3;b[$3]++}b[$3]>Mode{Mode=$3}END{print "Mean: " sum/x "\nMedian: "a[int((x-1)/2)]"\nMode: " Mode}' nanopore_readsDepth.tsv > median_output_nanopore_readsDepth.tsv
       
So I wrote a Python script to do it in a more efficient way and submitted it in a shell script (Below is an example of this for the Illumina data set)

       See the script: IlluminaMeanMedianMode.py (Below)
      
[IlluminaMeanMedianMode.py](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaMeanMedianMode.py)

       See the SHELL script that ran the python code: IlluminaMeanMedianMode.sh (Below)
      
[IlluminaMeanMedianMode.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/IlluminaMeanMedianMode.sh)

## Replacing BWA with MiniMap:

For the long read files I chose to use MiniMaP instead of BWA becuase it was stuggling with PacBio and potentially was truncating the alignment file 

       See the PacBio script: PacBioMiniMapandDepth.sh  (Below)
       
[PacBioMiniMapandDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/PacBioMiniMapandDepth.sh)

       See the Nanopore script: NanoporeMiniMapandDepth.sh  (Below)
       
[NanoporeMiniMapandDepth.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/NanoporeMiniMapandDepth.sh)


## Finding the depth at and around the duplicated region:

I wrote a Python script to return to me a TSV that shows the depth at and around the duplicated region

       See the script: depthAtDuplicatedRegionsHumanReadable.py  (Below)
      
[depthAtDuplicatedRegionsHumanReadable.py](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/depthAtDuplicatedRegionsHumanReadable.py)

       See the SHELL script that ran the python code: depthAtDuplicatedRegionsHumanReadable.sh  (Below)
      
[depthAtDuplicatedRegionsHumanReadable.sh](https://github.com/TroyLaPolice/Gene-Duplication-Origin-in-a-Non-Model-Organism/blob/master/depthAtDuplicatedRegionsHumanReadable.sh)

### The next section was written up into a Repo for Publishing:
## It is as follows:

### Analyses of duplicated orthologs

This document details our approach to examining why so many of the expected orthologs are present in duplicated copies.

`Minimap2` was used to align PacBio reads and `samtools depth` was used to calculate depth: 

```
#!/bin/bash
#SBATCH --partition=macmanes
#SBATCH -J bwaMini
#SBATCH --output PBMiniMap.log
#SBATCH --cpus-per-task=40
#SBATCH --mem=300000
#SBATCH --exclude=node117,node118

DIR="/mnt/lustre/macmaneslab/tml1019/seniorThesis/genomeFiles"
ASSEMBLY="/mnt/lustre/macmaneslab/tml1019/seniorThesis/genomeFiles/imitator.1.3.6.fa"
READS="/mnt/lustre/macmaneslab/ams1236/imitator_genome/reads/PacBio_reads.fa"

module purge
module load linuxbrew/colsa

cd /mnt/lustre/macmaneslab/tml1019/seniorThesis/BWAandDepth/PacBio_aligned_reads

# Run MiniMap becuase this is long read

minimap2 -x map-pb -I50g -N 10 -a -t 40 $ASSEMBLY $READS | samtools view -Sb - | samtools sort -T PacBio -O bam -@40 -l9 -m2G -o PacBioMiniMap.sorted.bam -

samtools index PacBioMiniMap.sorted.bam

# samtools depth
samtools depth -aa PacBioMiniMap.sorted.bam > PacBioMiniMapDepth.tsv
```

We calculated summary statistics (mean, median, mode, sd) on read depth:

```
#! /usr/bin/env python3

import statistics

# empty list
stats_list = []

try:
    with open("PacBioMiniMapDepth.tsv", "r") as input:
        for line in input:
            fields = line.split("\t")
            stats_list.append(int(fields[2]))
except IOError:
    print("problem reading file")

print("Mean:  ", statistics.mean(stats_list))
print("Median:  ", statistics.median(stats_list))
print("Mode:  ", statistics.mode(stats_list))
print("Stdv:  ", statistics.stdev(stats_list))
```

Pull out depth at specific duplicated regions:

```
#! /usr/bin/env python3

# *****************************************************
#   Script returns the depth at the bases inside the regions of intrest indicated in the BUSCO Output (duplicate/single copy)
# *****************************************************

#Initialize dictionary that will store duplicates
dupDictionary = dict()

#Read in file with list of genes from BUSCO output
with open("BUSCO4_duplicatedGenes.tsv", "r") as duplicatedGenes:
    for duplicatedLine in duplicatedGenes:
        duplicatedLine_stripped = duplicatedLine.strip()
        duplicate = duplicatedLine_stripped.split("\t")

        #Set the key to be the sacffold value
        key = duplicate[2]

        #If key doesnt exist in dictionary, make it
        dupDictionary.setdefault(key, [])

        #Get the BUSCO ID, the start and the end of the gene from the BUSCO output
        valuesToAppend = [duplicate[0], duplicate[3], duplicate[4]]

        #Add those values to the dictionary
        dupDictionary[key].append(valuesToAppend)

#Read in genome depth file

with open("PacBioMiniMapDepth.tsv", "r") as genomeDepth:
    for genomeLine in genomeDepth:
        genomeLine_stripped = genomeLine.strip()
        genome = genomeLine_stripped.split("\t")

        scaffold_name = (genome[0])
        coordinates = int(genome[1])

        #If the current line in the genome depth file contains a scaffold from the BUSCO file, report the depth at each base in that region of the genome, output to a TSV
        if scaffold_name in dupDictionary:

            for value in dupDictionary[scaffold_name]:
                endofDup = value[2]
                endofDup = int(endofDup)

                startofDup = value[1]
                startofDup = int(startofDup)

                # Get +- 5kb on either side of the gene of intrest
                lower = int(startofDup) - 50000
                upper = int(endofDup) + 50000

                if coordinates >= (lower) and coordinates <= (upper):
                    print(value[0], "\t", scaffold_name, "\t", coordinates, "\t", genome[2])
```

Calculate number of duplicated orthologs per scaffold:

```
#Load Packages

library(plyr)
library(dplyr)

#Read in File
duplicated_genes <- read.table("BUSCO4_duplicated.tsv", sep = "\t", header = T, stringsAsFactors = F)

head(duplicated_genes)
colnames(duplicated_genes) <- c("Busco_id", "Status", "Scaffold", "Start", "End", "Score", "Length", "OrthoDB", "Gene")

# *****************************************************
#   Count the number of duplicates on each Scaffold
# *****************************************************

scaffold_count <- count(duplicated_genes, Scaffold)

write.csv(scaffold_count, "scaffold_count.csv")
```

Calculate number of duplicated orthologs per scaffold normalized by scaffold length:

```
#! /usr/bin/env python3

# *****************************************************
#   Number of duplicates on each Scaffold Normalized by Scaffold Length
# *****************************************************

comboDictionary = dict()

#Read in file with the scaffold name and its length; input file is the index of the genome from samtools
with open("imitator.1.3.6.fa.fai", "r") as duplicatedGenes:
    for duplicatedLine in duplicatedGenes:
        duplicatedLine_stripped = duplicatedLine.strip()
        duplicate = duplicatedLine_stripped.split(",")

        #Key = scaffold name
        key = duplicate[0]

        #If key doesnt exist it makes one
        comboDictionary.setdefault(key, [])

        #Adds the scaffold length to the dictionary
        valuesToAppend = [duplicate[1]]
        comboDictionary[key].append(valuesToAppend)

#Reads in a file with the number of duplicates per scaffold
with open("scaffold_count.csv", "r") as scaffoldcount:
    for line in scaffoldcount:
        line_stripped = line.strip()
        scaffold = line_stripped.split(",")

        key = scaffold[0]

        comboDictionary.setdefault(key, [])

        #Adds the number of duplicates by scaffold
        valuesToAppend = [scaffold[1]]
        comboDictionary[key].append(valuesToAppend)

percentagePerScafDict = dict()

for key in comboDictionary:

    #takes the number of duplicates and normalizes it by scaffold length
    entry1and2 = []
    for entry in comboDictionary[key]:
        for line in entry:
            entry1and2.append(line)

    try:
        print(key, "\t", int(entry1and2[1])/int(entry1and2[0]))
    except:
        continue
```

Do both copies of orthologs ever appear on the same scaffold?

```
#! /usr/bin/env python3

# *****************************************************
#   If the duplicate and its complement are on the same scaffold it will print the BUSCO ID and the scaffold that they appear on
# *****************************************************

dupDictionary = dict()

#Read in BUSCO Output
with open("BUSCO4_duplicated.tsv", "r") as duplicatedGenes:
    for duplicatedLine in duplicatedGenes:
        duplicatedLine_stripped = duplicatedLine.strip()
        duplicate = duplicatedLine_stripped.split("\t")

        #key = BUSCO ID
        key = duplicate[0]

        #If key doesnt already exist, make it
        dupDictionary.setdefault(key, [])

        #Gets the scaffold of the duplicate
        valuesToAppend = [duplicate[2]]

        #Adds it to dictionary
        dupDictionary[key].append(valuesToAppend)

    for key in dupDictionary:

        #Initialize a list to capture the two values
        entry1and2 = []
        for entry in dupDictionary[key]:
            for line in entry:
                entry1and2.append(line)


        #If the duplicate and its complement are on the same scaffold it will print the BUSCO ID and the scaffold it is on
        #If not, it will break the loop and move on

        item = entry1and2[0]
        check = True
        for i in entry1and2:
            if item != i:
                check = False
                break
        if (check == True):
            print(key, dupDictionary[key])
```

We then calculated sliding window statistics for the data using an [R script that we ran on the cluster](https://github.com/AdamStuckert/Ranitomeya_imitator_genome/blob/master/GenomeAssembly/scripts/BUSCO4_Dups.R). The results of this script were then used to compare orthologs present in single copies and duplicated copies in a [subsequent R script which we ran locally](https://github.com/AdamStuckert/Ranitomeya_imitator_genome/blob/master/GenomeAssembly/scripts/GenomeDuplicates.Rmd).
